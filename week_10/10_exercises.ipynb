{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural & Behavioral Modeling - Week 10 (Exercises)\n",
    "Howard Chao (ntueeb05howard@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True \n",
    "%matplotlib inline\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "from IPython.display import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 A Two-layered Linear Network as a Regression Model (7 points)\n",
    "Data fitting of the following network is poor. Please check if adding bias terms or chaging network hyperparameters (e.g., learning rate, amount of training, etc.) help. If not, please explain why the fitting is poor given that the network/regression model has sufficient degrees of freedom (i.e., network weights or regression coefficients) to overfit such a small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_printoptions(precision=3,suppress=True)\n",
    "X=array([[1,0,0,0],[0,1,0,0],[1,1,0,0],[0,0,1,0],[0,0,0,1],[0,0,1,1]])\n",
    "Y=array([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1]])\n",
    "[Np,Nx]=X.shape; # find numbers of patterns and input dimensions\n",
    "[Np,Ny]=Y.shape; # find numbers of patterns and output dimensions\n",
    "W=random.rand(Ny,Nx); # set initially random connectivity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.677,  0.203,  0.944,  0.559],\n",
       "       [ 0.626,  0.285,  0.128,  0.604]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin W: \n",
      " [[ 0.527  0.003  0.438  0.744]\n",
      " [ 0.914  0.063  0.83   0.629]]\n",
      "Ideal results:\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Reality:\n",
      "[[ 0.679  0.   ]\n",
      " [ 0.643 -0.   ]\n",
      " [ 1.321  0.   ]\n",
      " [-0.     0.643]\n",
      " [ 0.     0.643]\n",
      " [ 0.     1.286]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11391cda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we train a two-layered network of units \n",
    "# with a linear activation function f(x)=x\n",
    "# to associate patterns using the delta rule dW=(t-y)*x\n",
    "\n",
    "set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "X=array([[1,0,0,0],[0,1,0,0],[1,1,0,0],[0,0,1,0],[0,0,0,1],[0,0,1,1]])\n",
    "Y=array([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1]])\n",
    "[Np,Nx]=X.shape; # find numbers of patterns and input dimensions\n",
    "[Np,Ny]=Y.shape; # find numbers of patterns and output dimensions\n",
    "W=random.rand(Ny,Nx); # set initially random connectivity matrix\n",
    "\n",
    "eta=.1; # set the learning rate \n",
    "tol=1e-2; # set the tolerance/stopping criterion; try 0.01\n",
    "nIts=50000; # set the maximum number of allowed iterations\n",
    "totErr=10; # set the maximum training error to an initially high value\n",
    "totErr_hist=[] # history of totall error\n",
    "\n",
    "print(\"Origin W: \\n\", W)\n",
    "\n",
    "for c in range(nIts): # for each learning iteration\n",
    "    p=mod(c,Np) # sequential presentation of the training samples (0 ~ 5)\n",
    "#     p=random.randint(nP); # choose a traing pattern at random\n",
    "    \n",
    "    # Forward propagation:\n",
    "    y=W.dot(X[p])\n",
    "#     print(\"y: \", y)\n",
    "    \n",
    "    # Backward propagation:\n",
    "    deltaW=eta*outer(Y[p].T-y,X[p]) # delta learning\n",
    "#     print(outer(Y[p].T-y,X[p]))\n",
    "    W=W+deltaW;  # apply the weight update\n",
    "#     print(c)\n",
    "#     print(W)\n",
    "    \n",
    "    # Checking if done:\n",
    "    if(mod(c,10*Np)==0): # after 10 updates check total errors (0, 60 ,120 ...)\n",
    "        predY=W.dot(X.T) # testing ALL the training samples (X transform 4*6)\n",
    "#         print(\"W : \\n\", W)\n",
    "#         print(\"X : \\n\", X.T)\n",
    "#         print(\"predY:\")\n",
    "#         print(\"Normal: \")\n",
    "#         print((Y.T-predY))\n",
    "#         print(\"Square: \")\n",
    "#         print((Y.T-predY)**2)\n",
    "#         print(predY.T) # predicted Y\n",
    "        totErr=sum((Y.T-predY)**2) # sum of squared errors for all samples\n",
    "        totErr_hist.append(totErr)\n",
    "#         print(\"totErr: \", totErr)\n",
    "    if(totErr<tol):\n",
    "        break # break if max error is below tolerance\n",
    "# print(\"len\", len(totErr_hist))\n",
    "plot(totErr_hist);\n",
    "title(str(c)+':'+str(totErr));\n",
    "print('Ideal results:')\n",
    "print(Y)\n",
    "print('Reality:')\n",
    "print(predY.T) # predicted Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 說明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 測試結果大概到 360 多次就已經到達訓練的極限，無法再增加準確度，故增加訓練次數對於準確度的提升沒有影響。\n",
    "2. 嘗試調整 eta=0.1 (learning rate)，雖然有改變訓練的結果，但是預測的結果卻變得不準確。（因為eta變大，**delta learning** 變快，大 'deltaW=eta*outer(Y[p].T-y,X[p])'， backpropagation 的修正項變大）\n",
    "3. 本次因為資料的問題，所以無法透過 regression model 來接近所要的結果（接近Y），解釋原因如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.819,  0.655,  0.108])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 結果Ｗ\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'X' 是一個 6*4 的矩陣，而所要預測的 'Y' 是一個 6*2 的矩陣。\n",
    "可以利用線性代數的方法解釋，可以試 X 為 六個聯立方程式。我們看前三個連立方程式：\n",
    "\n",
    "'x = 1'\n",
    "\n",
    "'y = 1'\n",
    "\n",
    "'x + y = 1'\n",
    "\n",
    "'a = 1'\n",
    "\n",
    "'b = 1'\n",
    "\n",
    "'a + b = 1'\n",
    "\n",
    "不管由上面三式，或者下面三式，都無法找到解，所以這題無法使用一般的方法訓練出接近 'Y'(預期結果的答案)。\n",
    "加入修正項，來嘗試逼近結果(也無法，第三項依舊為第一、二項的和)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin W: \n",
      " [[ 0.888  0.15   0.805  0.964]\n",
      " [ 0.585  0.561  0.826  0.025]]\n",
      "Ideal results:\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Reality:\n",
      "[[ 0.543 -0.136]\n",
      " [ 0.514 -0.129]\n",
      " [ 1.057 -0.264]\n",
      " [-0.129  0.514]\n",
      " [-0.129  0.514]\n",
      " [-0.257  1.029]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e7e16a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we train a two-layered network of units \n",
    "# with a linear activation function f(x)=x\n",
    "# to associate patterns using the delta rule dW=(t-y)*x\n",
    "\n",
    "set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "X=array([[1,0,0,0],[0,1,0,0],[1,1,0,0],[0,0,1,0],[0,0,0,1],[0,0,1,1]])\n",
    "Y=array([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1]])\n",
    "[Np,Nx]=X.shape; # find numbers of patterns and input dimensions\n",
    "[Np,Ny]=Y.shape; # find numbers of patterns and output dimensions\n",
    "W=random.rand(Ny,Nx); # set initially random connectivity matrix\n",
    "\n",
    "eta=.1; # set the learning rate \n",
    "tol=1e-2; # set the tolerance/stopping criterion; try 0.01\n",
    "nIts=50000; # set the maximum number of allowed iterations\n",
    "totErr=10; # set the maximum training error to an initially high value\n",
    "totErr_hist=[] # history of totall error\n",
    "\n",
    "print(\"Origin W: \\n\", W)\n",
    "\n",
    "for c in range(nIts): # for each learning iteration\n",
    "    p=mod(c,Np) # sequential presentation of the training samples (0 ~ 5)\n",
    "#     p=random.randint(nP); # choose a traing pattern at random\n",
    "    \n",
    "    # Forward propagation:\n",
    "    fixing_term = [0.2, 0.2]\n",
    "    y=W.dot(X[p]) + fixing_term\n",
    "#     print(\"y: \", y)\n",
    "    \n",
    "    # Backward propagation:\n",
    "    deltaW=eta*outer(Y[p].T-y,X[p]) # delta learning\n",
    "#     print(outer(Y[p].T-y,X[p]))\n",
    "    W=W+deltaW;  # apply the weight update\n",
    "#     print(c)\n",
    "#     print(W)\n",
    "    \n",
    "    # Checking if done:\n",
    "    if(mod(c,10*Np)==0): # after 10 updates check total errors (0, 60 ,120 ...)\n",
    "        predY=W.dot(X.T) # testing ALL the training samples (X transform 4*6)\n",
    "#         print(\"W : \\n\", W)\n",
    "#         print(\"X : \\n\", X.T)\n",
    "#         print(\"predY:\")\n",
    "#         print(\"Normal: \")\n",
    "#         print((Y.T-predY))\n",
    "#         print(\"Square: \")\n",
    "#         print((Y.T-predY)**2)\n",
    "#         print(predY.T) # predicted Y\n",
    "        totErr=sum((Y.T-predY)**2) # sum of squared errors for all samples\n",
    "        totErr_hist.append(totErr)\n",
    "#         print(\"totErr: \", totErr)\n",
    "    if(totErr<tol):\n",
    "        break # break if max error is below tolerance\n",
    "# print(\"len\", len(totErr_hist))\n",
    "plot(totErr_hist);\n",
    "title(str(c)+':'+str(totErr));\n",
    "print('Ideal results:')\n",
    "print(Y)\n",
    "print('Reality:')\n",
    "print(predY.T) # predicted Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 PyTorch (3 points)\n",
    "Read <a href=\"http://noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94Numpy%EF%BC%8CTorch%E5%AF%B9%E6%AF%94/\">this tutorial</a> first and port the following Instar Learning from NumPy to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.424  0.961  0.769] 0.71495969147\n",
      "[ 0.192  0.488  0.577] 0.454028223058\n",
      "[ 0.15   0.403  0.542] 0.406796473795\n",
      "[ 0.13   0.361  0.525] 0.383691868531\n",
      "[ 0.118  0.338  0.515] 0.37076457254\n",
      "[ 0.112  0.324  0.51 ] 0.363065804678\n",
      "[ 0.107  0.315  0.506] 0.358322057789\n",
      "[ 0.105  0.31   0.504] 0.355340080917\n",
      "[ 0.103  0.306  0.503] 0.353442536132\n",
      "[ 0.102  0.304  0.502] 0.352225797431\n"
     ]
    }
   ],
   "source": [
    "# Instar learning:\n",
    "x=array([0.1,0.3,0.5])\n",
    "W=random.rand(3)\n",
    "for i in range(10): # trials \n",
    "    y=dot(W,x)\n",
    "    print(W,y)\n",
    "    W+=y*(x-W) # Postsynaptically gated InStar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your PyTorch codes here\n",
    "import torch as t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8192,  0.6548,  0.1083]) tensor(0.3325)\n",
      "tensor([ 0.5801,  0.5368,  0.2386]) tensor(0.6709)\n",
      "tensor([ 0.2580,  0.3779,  0.4140]) tensor(1.0170)\n",
      "tensor([ 0.0973,  0.2987,  0.5015]) tensor(1.3671)\n",
      "tensor([ 0.1010,  0.3005,  0.4995]) tensor(1.7171)\n",
      "tensor([ 0.0993,  0.2997,  0.5004]) tensor(2.0671)\n",
      "tensor([ 0.1008,  0.3004,  0.4996]) tensor(2.4171)\n",
      "tensor([ 0.0989,  0.2995,  0.5006]) tensor(2.7671)\n",
      "tensor([ 0.1019,  0.3009,  0.4990]) tensor(3.1171)\n"
     ]
    }
   ],
   "source": [
    "x=array([0.1,0.3,0.5])\n",
    "torch_data = t.FloatTensor(x)\n",
    "W=random.rand(3)\n",
    "torch_random = t.FloatTensor(W)\n",
    "dot_result = 0\n",
    "for i in range(1,10):\n",
    "    for i in range(0,3):\n",
    "        dot_result += torch_random[i]*torch_data[i]\n",
    "#     dot_result = torch_random.dot(torch_data)\n",
    "    print(torch_random, dot_result)\n",
    "    torch_random += dot_result * (torch_data - torch_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0960,  0.2980,  0.5022])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal conversion between array and tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(6).reshape((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy的array与torch的tensor的转换\n",
    "np_data = np.arange(6).reshape((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data = t.from_numpy(np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2array = torch_data.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy data:  [[0 1 2]\n",
      " [3 4 5]]\n",
      "torch data:  tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5]])\n",
      "tensor2array:  [[0 1 2]\n",
      " [3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "print('numpy data: ', np_data)\n",
    "print('torch data: ', torch_data)\n",
    "print('tensor2array: ', tensor2array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., -1.,  0.,  1.,  2.])\n"
     ]
    }
   ],
   "source": [
    "data = [-2, -1, 0, 1, 2]\n",
    "float_data = t.FloatTensor(data)\n",
    "print(float_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to abs tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  1.,  0.,  1.,  2.])\n"
     ]
    }
   ],
   "source": [
    "print(t.abs(float_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.90929743 -0.84147098  0.          0.84147098  0.90929743]\n",
      "tensor([-0.9093, -0.8415,  0.0000,  0.8415,  0.9093])\n"
     ]
    }
   ],
   "source": [
    "print(np.sin(data))\n",
    "print(t.sin(float_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(data))\n",
    "print(t.mean(float_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 10]\n",
      " [15 22]]\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "tensor = t.FloatTensor(data)\n",
    "print(np.matmul(data, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  7.,  10.],\n",
      "        [ 15.,  22.]])\n",
      "tensor([[  7.,  10.],\n",
      "        [ 15.,  22.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.mm不支持广播形式\n",
    "print(t.mm(tensor, tensor))\n",
    "# torch.matmul支持广播形式\n",
    "print(t.matmul(tensor, tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
